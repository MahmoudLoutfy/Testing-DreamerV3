{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium==1.0.0 ale-py==0.10.1 --quiet\n\n!pip install \"protobuf<5.0\" --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:06:35.394545Z","iopub.execute_input":"2025-12-24T08:06:35.394879Z","iopub.status.idle":"2025-12-24T08:06:44.163967Z","shell.execute_reply.started":"2025-12-24T08:06:35.394854Z","shell.execute_reply":"2025-12-24T08:06:44.163082Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=55) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngrain 0.2.15 requires protobuf>=5.28.3, but you have protobuf 4.25.8 which is incompatible.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport random\nimport time\nfrom dataclasses import dataclass\n\nimport gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\nimport ale_py\ngym.register_envs(ale_py)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:06:53.666050Z","iopub.execute_input":"2025-12-24T08:06:53.666598Z","iopub.status.idle":"2025-12-24T08:06:53.745211Z","shell.execute_reply.started":"2025-12-24T08:06:53.666562Z","shell.execute_reply":"2025-12-24T08:06:53.744092Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from __future__ import annotations\n\nfrom typing import SupportsFloat\n\nimport gymnasium as gym\nimport numpy as np\nfrom gymnasium import spaces\n\ntry:\n    import cv2\n\n    cv2.ocl.setUseOpenCL(False)\nexcept ImportError:\n    cv2 = None  # type: ignore[assignment]\n\nclass NoopResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n    \"\"\"\n    Sample initial states by taking random number of no-ops on reset.\n    No-op is assumed to be action 0.\n\n    :param env: Environment to wrap\n    :param noop_max: Maximum value of no-ops to run\n    \"\"\"\n\n    def __init__(self, env: gym.Env, noop_max: int = 30) -> None:\n        super().__init__(env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"  # type: ignore[attr-defined]\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n        assert noops > 0\n        obs = np.zeros(0)\n        info: dict = {}\n        for _ in range(noops):\n            obs, _, terminated, truncated, info = self.env.step(self.noop_action)\n            if terminated or truncated:\n                obs, info = self.env.reset(**kwargs)\n        return obs, info\n\n\nclass FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n    \"\"\"\n    Take action on reset for environments that are fixed until firing.\n\n    :param env: Environment to wrap\n    \"\"\"\n\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"  # type: ignore[attr-defined]\n        assert len(env.unwrapped.get_action_meanings()) >= 3  # type: ignore[attr-defined]\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, terminated, truncated, _ = self.env.step(1)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        obs, _, terminated, truncated, _ = self.env.step(2)\n        if terminated or truncated:\n            self.env.reset(**kwargs)\n        return obs, {}\n\n\nclass EpisodicLifeEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n    \"\"\"\n    Make end-of-life == end-of-episode, but only reset on true game over.\n    Done by DeepMind for the DQN and co. since it helps value estimation.\n\n    :param env: Environment to wrap\n    \"\"\"\n\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action: int):\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        self.was_real_done = terminated or truncated\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n        if 0 < lives < self.lives:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            terminated = True\n        self.lives = lives\n        return obs, reward, terminated, truncated, info\n\n    def reset(self, **kwargs):\n        \"\"\"\n        Calls the Gym environment reset, only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n\n        :param kwargs: Extra keywords passed to env.reset() call\n        :return: the first observation of the environment\n        \"\"\"\n        if self.was_real_done:\n            obs, info = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, terminated, truncated, info = self.env.step(0)\n\n            # The no-op step can lead to a game over, so we need to check it again\n            # to see if we should reset the environment and avoid the\n            # monitor.py `RuntimeError: Tried to step environment that needs reset`\n            if terminated or truncated:\n                obs, info = self.env.reset(**kwargs)\n        self.lives = self.env.unwrapped.ale.lives()  # type: ignore[attr-defined]\n        return obs, info\n\n\nclass MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]):\n    \"\"\"\n    Return only every ``skip``-th frame (frameskipping)\n    and return the max between the two last frames.\n\n    :param env: Environment to wrap\n    :param skip: Number of ``skip``-th frame\n        The same action will be taken ``skip`` times.\n    \"\"\"\n\n    def __init__(self, env: gym.Env, skip: int = 4) -> None:\n        super().__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        assert env.observation_space.dtype is not None, \"No dtype specified for the observation space\"\n        assert env.observation_space.shape is not None, \"No shape defined for the observation space\"\n        self._obs_buffer = np.zeros((2, *env.observation_space.shape), dtype=env.observation_space.dtype)\n        self._skip = skip\n\n    def step(self, action: int):\n        \"\"\"\n        Step the environment with the given action\n        Repeat action, sum reward, and max over last observations.\n\n        :param action: the action\n        :return: observation, reward, terminated, truncated, information\n        \"\"\"\n        total_reward = 0.0\n        terminated = truncated = False\n        for i in range(self._skip):\n            obs, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += float(reward)\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, terminated, truncated, info\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    \"\"\"\n    Clip the reward to {+1, 0, -1} by its sign.\n\n    :param env: Environment to wrap\n    \"\"\"\n\n    def __init__(self, env: gym.Env) -> None:\n        super().__init__(env)\n\n    def reward(self, reward: SupportsFloat) -> float:\n        \"\"\"\n        Bin reward to {+1, 0, -1} by its sign.\n\n        :param reward:\n        :return:\n        \"\"\"\n        return np.sign(float(reward))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:58:56.268912Z","iopub.execute_input":"2025-12-24T07:58:56.269652Z","iopub.status.idle":"2025-12-24T07:58:56.638317Z","shell.execute_reply.started":"2025-12-24T07:58:56.269622Z","shell.execute_reply":"2025-12-24T07:58:56.637078Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"@dataclass\nclass Args:\n    exp_name: str = \"sac_atari_spaceinvaders\"\n    seed: int = 1\n    torch_deterministic: bool = True\n    cuda: bool = True\n    track: bool = True  # Enable wandb tracking\n    wandb_project_name: str = \"Assignment5-v5-running\"\n    wandb_entity: str = \"amira-elgarf02\"  # Add your wandb username\n    capture_video: bool = True  # Enable video recording\n    \n    # Environment\n    env_id: str = \"SpaceInvadersNoFrameskip-v4\"\n    total_timesteps: int = 3000000  \n    \n    # Algorithm parameters\n    buffer_size: int = int(1e5)\n    gamma: float = 0.99\n    tau: float = 1.0\n    batch_size: int = 64\n    learning_starts: int = 10000\n    policy_lr: float = 3e-4\n    q_lr: float = 3e-4\n    update_frequency: int = 4\n    target_network_frequency: int = 8000\n    alpha: float = 0.2\n    autotune: bool = True\n    target_entropy_scale: float = 0.89","metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        if capture_video and idx == 0:\n            env = gym.make(env_id, render_mode=\"rgb_array\")\n            env = gym.wrappers.RecordVideo(\n                env, \n                f\"videosCopy/{run_name}\",\n                episode_trigger=lambda x: x % 100 == 0  # Record every 100 episodes\n            )\n        else:\n            env = gym.make(env_id)\n        \n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        \n        # Atari preprocessing\n        env = NoopResetEnv(env, noop_max=30)\n        env = MaxAndSkipEnv(env, skip=4)\n        env = EpisodicLifeEnv(env)\n        if \"FIRE\" in env.unwrapped.get_action_meanings():\n            env = FireResetEnv(env)\n        env = ClipRewardEnv(env)\n        env = gym.wrappers.ResizeObservation(env, (84, 84))\n        env = gym.wrappers.GrayscaleObservation(env)\n        env = gym.wrappers.FrameStackObservation(env, 4)\n        \n        env.action_space.seed(seed)\n        return env\n    \n    return thunk\n\n\ndef layer_init(layer, bias_const=0.0):\n    nn.init.kaiming_normal_(layer.weight)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\n\nclass SoftQNetwork(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        obs_shape = envs.single_observation_space.shape\n        self.conv = nn.Sequential(\n            layer_init(nn.Conv2d(obs_shape[0], 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.Flatten(),\n        )\n        \n        with torch.inference_mode():\n            output_dim = self.conv(torch.zeros(1, *obs_shape)).shape[1]\n        \n        self.fc1 = layer_init(nn.Linear(output_dim, 512))\n        self.fc_q = layer_init(nn.Linear(512, envs.single_action_space.n))\n    \n    def forward(self, x):\n        x = F.relu(self.conv(x / 255.0))\n        x = F.relu(self.fc1(x))\n        q_vals = self.fc_q(x)\n        return q_vals\n\n\nclass Actor(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        obs_shape = envs.single_observation_space.shape\n        self.conv = nn.Sequential(\n            layer_init(nn.Conv2d(obs_shape[0], 32, kernel_size=8, stride=4)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n            nn.ReLU(),\n            layer_init(nn.Conv2d(64, 64, kernel_size=3, stride=1)),\n            nn.Flatten(),\n        )\n        \n        with torch.inference_mode():\n            output_dim = self.conv(torch.zeros(1, *obs_shape)).shape[1]\n        \n        self.fc1 = layer_init(nn.Linear(output_dim, 512))\n        self.fc_logits = layer_init(nn.Linear(512, envs.single_action_space.n))\n    \n    def forward(self, x):\n        x = F.relu(self.conv(x))\n        x = F.relu(self.fc1(x))\n        logits = self.fc_logits(x)\n        return logits\n    \n    def get_action(self, x):\n        logits = self(x / 255.0)\n        policy_dist = Categorical(logits=logits)\n        action = policy_dist.sample()\n        action_probs = policy_dist.probs\n        log_prob = F.log_softmax(logits, dim=1)\n        return action, log_prob, action_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:59:01.076590Z","iopub.execute_input":"2025-12-24T07:59:01.076898Z","iopub.status.idle":"2025-12-24T07:59:01.093398Z","shell.execute_reply.started":"2025-12-24T07:59:01.076876Z","shell.execute_reply":"2025-12-24T07:59:01.092527Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from __future__ import annotations\n\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Generator\nfrom typing import Any, NamedTuple\n\nimport numpy as np\nimport torch as th\nfrom gymnasium import spaces\n\ntry:\n    # Check memory used by replay buffer when possible\n    import psutil\nexcept ImportError:\n    psutil = None\n\nclass ReplayBufferSamples(NamedTuple):\n    observations: th.Tensor\n    actions: th.Tensor\n    next_observations: th.Tensor\n    dones: th.Tensor\n    rewards: th.Tensor\n\n\ndef get_action_dim(action_space: spaces.Space) -> int:\n    \"\"\"\n    Get the dimension of the action space.\n\n    :param action_space:\n    :return:\n    \"\"\"\n    if isinstance(action_space, spaces.Box):\n        return int(np.prod(action_space.shape))\n    elif isinstance(action_space, spaces.Discrete):\n        # Action is an int\n        return 1\n    elif isinstance(action_space, spaces.MultiDiscrete):\n        # Number of discrete actions\n        return int(len(action_space.nvec))\n    elif isinstance(action_space, spaces.MultiBinary):\n        # Number of binary actions\n        assert isinstance(\n            action_space.n, int\n        ), f\"Multi-dimensional MultiBinary({action_space.n}) action space is not supported. You can flatten it instead.\"\n        return int(action_space.n)\n    else:\n        raise NotImplementedError(f\"{action_space} action space is not supported\")\n\n\ndef get_obs_shape(\n    observation_space: spaces.Space,\n) -> tuple[int, ...] | dict[str, tuple[int, ...]]:\n    \"\"\"\n    Get the shape of the observation (useful for the buffers).\n\n    :param observation_space:\n    :return:\n    \"\"\"\n    if isinstance(observation_space, spaces.Box):\n        return observation_space.shape\n    elif isinstance(observation_space, spaces.Discrete):\n        # Observation is an int\n        return (1,)\n    elif isinstance(observation_space, spaces.MultiDiscrete):\n        # Number of discrete features\n        return (int(len(observation_space.nvec)),)\n    elif isinstance(observation_space, spaces.MultiBinary):\n        # Number of binary features\n        return observation_space.shape\n    elif isinstance(observation_space, spaces.Dict):\n        return {key: get_obs_shape(subspace) for (key, subspace) in observation_space.spaces.items()}  # type: ignore[misc]\n\n    else:\n        raise NotImplementedError(f\"{observation_space} observation space is not supported\")\n\n\ndef get_device(device: th.device | str = \"auto\") -> th.device:\n    \"\"\"\n    Retrieve PyTorch device.\n    It checks that the requested device is available first.\n    For now, it supports only cpu and cuda.\n    By default, it tries to use the gpu.\n\n    :param device: One for 'auto', 'cuda', 'cpu'\n    :return: Supported Pytorch device\n    \"\"\"\n    # Cuda by default\n    if device == \"auto\":\n        device = \"cuda\"\n    # Force conversion to th.device\n    device = th.device(device)\n\n    # Cuda not available\n    if device.type == th.device(\"cuda\").type and not th.cuda.is_available():\n        return th.device(\"cpu\")\n\n    return device\n\nclass BaseBuffer(ABC):\n    \"\"\"\n    Base class that represent a buffer (rollout or replay)\n\n    :param buffer_size: Max number of element in the buffer\n    :param observation_space: Observation space\n    :param action_space: Action space\n    :param device: PyTorch device\n        to which the values will be converted\n    :param n_envs: Number of parallel environments\n    \"\"\"\n\n    observation_space: spaces.Space\n    obs_shape: tuple[int, ...]\n\n    def __init__(\n        self,\n        buffer_size: int,\n        observation_space: spaces.Space,\n        action_space: spaces.Space,\n        device: th.device | str = \"auto\",\n        n_envs: int = 1,\n    ):\n        super().__init__()\n        self.buffer_size = buffer_size\n        self.observation_space = observation_space\n        self.action_space = action_space\n        self.obs_shape = get_obs_shape(observation_space)  # type: ignore[assignment]\n\n        self.action_dim = get_action_dim(action_space)\n        self.pos = 0\n        self.full = False\n        self.device = get_device(device)\n        self.n_envs = n_envs\n\n    @staticmethod\n    def swap_and_flatten(arr: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Swap and then flatten axes 0 (buffer_size) and 1 (n_envs)\n        to convert shape from [n_steps, n_envs, ...] (when ... is the shape of the features)\n        to [n_steps * n_envs, ...] (which maintain the order)\n\n        :param arr:\n        :return:\n        \"\"\"\n        shape = arr.shape\n        if len(shape) < 3:\n            shape = (*shape, 1)\n        return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n\n    def size(self) -> int:\n        \"\"\"\n        :return: The current size of the buffer\n        \"\"\"\n        if self.full:\n            return self.buffer_size\n        return self.pos\n\n    def add(self, *args, **kwargs) -> None:\n        \"\"\"\n        Add elements to the buffer.\n        \"\"\"\n        raise NotImplementedError()\n\n    def extend(self, *args, **kwargs) -> None:\n        \"\"\"\n        Add a new batch of transitions to the buffer\n        \"\"\"\n        # Do a for loop along the batch axis\n        for data in zip(*args):\n            self.add(*data)\n\n    def reset(self) -> None:\n        \"\"\"\n        Reset the buffer.\n        \"\"\"\n        self.pos = 0\n        self.full = False\n\n    def sample(self, batch_size: int):\n        \"\"\"\n        :param batch_size: Number of element to sample\n        :return:\n        \"\"\"\n        upper_bound = self.buffer_size if self.full else self.pos\n        batch_inds = np.random.randint(0, upper_bound, size=batch_size)\n        return self._get_samples(batch_inds)\n\n    @abstractmethod\n    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples | RolloutBufferSamples:\n        \"\"\"\n        :param batch_inds:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n    def to_torch(self, array: np.ndarray, copy: bool = True) -> th.Tensor:\n        \"\"\"\n        Convert a numpy array to a PyTorch tensor.\n        Note: it copies the data by default\n\n        :param array:\n        :param copy: Whether to copy or not the data (may be useful to avoid changing things\n            by reference). This argument is inoperative if the device is not the CPU.\n        :return:\n        \"\"\"\n        if copy:\n            return th.tensor(array, device=self.device)\n        return th.as_tensor(array, device=self.device)\n\n\nclass ReplayBuffer(BaseBuffer):\n    \"\"\"\n    Replay buffer used in off-policy algorithms like SAC/TD3.\n\n    :param buffer_size: Max number of element in the buffer\n    :param observation_space: Observation space\n    :param action_space: Action space\n    :param device: PyTorch device\n    :param n_envs: Number of parallel environments\n    :param optimize_memory_usage: Enable a memory efficient variant\n        of the replay buffer which reduces by almost a factor two the memory used,\n        at a cost of more complexity.\n        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n        and https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n        Cannot be used in combination with handle_timeout_termination.\n    :param handle_timeout_termination: Handle timeout termination (due to timelimit)\n        separately and treat the task as infinite horizon task.\n        https://github.com/DLR-RM/stable-baselines3/issues/284\n    \"\"\"\n\n    observations: np.ndarray\n    next_observations: np.ndarray\n    actions: np.ndarray\n    rewards: np.ndarray\n    dones: np.ndarray\n    timeouts: np.ndarray\n\n    def __init__(\n        self,\n        buffer_size: int,\n        observation_space: spaces.Space,\n        action_space: spaces.Space,\n        device: th.device | str = \"auto\",\n        n_envs: int = 1,\n        optimize_memory_usage: bool = False,\n        handle_timeout_termination: bool = True,\n    ):\n        super().__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n\n        # Adjust buffer size\n        self.buffer_size = max(buffer_size // n_envs, 1)\n\n        # Check that the replay buffer can fit into the memory\n        if psutil is not None:\n            mem_available = psutil.virtual_memory().available\n\n        # there is a bug if both optimize_memory_usage and handle_timeout_termination are true\n        # see https://github.com/DLR-RM/stable-baselines3/issues/934\n        if optimize_memory_usage and handle_timeout_termination:\n            raise ValueError(\n                \"ReplayBuffer does not support optimize_memory_usage = True \"\n                \"and handle_timeout_termination = True simultaneously.\"\n            )\n        self.optimize_memory_usage = optimize_memory_usage\n\n        self.observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n\n        if not optimize_memory_usage:\n            # When optimizing memory, `observations` contains also the next observation\n            self.next_observations = np.zeros((self.buffer_size, self.n_envs, *self.obs_shape), dtype=observation_space.dtype)\n\n        self.actions = np.zeros(\n            (self.buffer_size, self.n_envs, self.action_dim), dtype=self._maybe_cast_dtype(action_space.dtype)\n        )\n\n        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        self.dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n        # Handle timeouts termination properly if needed\n        # see https://github.com/DLR-RM/stable-baselines3/issues/284\n        self.handle_timeout_termination = handle_timeout_termination\n        self.timeouts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\n        if psutil is not None:\n            total_memory_usage: float = (\n                self.observations.nbytes + self.actions.nbytes + self.rewards.nbytes + self.dones.nbytes\n            )\n\n            if not optimize_memory_usage:\n                total_memory_usage += self.next_observations.nbytes\n\n            if total_memory_usage > mem_available:\n                # Convert to GB\n                total_memory_usage /= 1e9\n                mem_available /= 1e9\n                warnings.warn(\n                    \"This system does not have apparently enough memory to store the complete \"\n                    f\"replay buffer {total_memory_usage:.2f}GB > {mem_available:.2f}GB\"\n                )\n\n    def add(\n        self,\n        obs: np.ndarray,\n        next_obs: np.ndarray,\n        action: np.ndarray,\n        reward: np.ndarray,\n        done: np.ndarray,\n        infos: list[dict[str, Any]],\n    ) -> None:\n        # Reshape needed when using multiple envs with discrete observations\n        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n        if isinstance(self.observation_space, spaces.Discrete):\n            obs = obs.reshape((self.n_envs, *self.obs_shape))\n            next_obs = next_obs.reshape((self.n_envs, *self.obs_shape))\n\n        # Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\n        action = action.reshape((self.n_envs, self.action_dim))\n\n        # Copy to avoid modification by reference\n        self.observations[self.pos] = np.array(obs)\n\n        if self.optimize_memory_usage:\n            self.observations[(self.pos + 1) % self.buffer_size] = np.array(next_obs)\n        else:\n            self.next_observations[self.pos] = np.array(next_obs)\n\n        self.actions[self.pos] = np.array(action)\n        self.rewards[self.pos] = np.array(reward)\n        self.dones[self.pos] = np.array(done)\n\n        if self.handle_timeout_termination:\n            self.timeouts[self.pos] = np.array([info.get(\"TimeLimit.truncated\", False) for info in infos])\n\n        self.pos += 1\n        if self.pos == self.buffer_size:\n            self.full = True\n            self.pos = 0\n\n    def sample(self, batch_size: int) -> ReplayBufferSamples:\n        \"\"\"\n        Sample elements from the replay buffer.\n        Custom sampling when using memory efficient variant,\n        as we should not sample the element with index `self.pos`\n        See https://github.com/DLR-RM/stable-baselines3/pull/28#issuecomment-637559274\n\n        :param batch_size: Number of element to sample\n        :return:\n        \"\"\"\n        if not self.optimize_memory_usage:\n            return super().sample(batch_size=batch_size)\n        # Do not sample the element with index `self.pos` as the transitions is invalid\n        # (we use only one array to store `obs` and `next_obs`)\n        if self.full:\n            batch_inds = (np.random.randint(1, self.buffer_size, size=batch_size) + self.pos) % self.buffer_size\n        else:\n            batch_inds = np.random.randint(0, self.pos, size=batch_size)\n        return self._get_samples(batch_inds)\n\n    def _get_samples(self, batch_inds: np.ndarray) -> ReplayBufferSamples:\n        # Sample randomly the env idx\n        env_indices = np.random.randint(0, high=self.n_envs, size=(len(batch_inds),))\n\n        if self.optimize_memory_usage:\n            next_obs = self.observations[(batch_inds + 1) % self.buffer_size, env_indices, :]\n        else:\n            next_obs = self.next_observations[batch_inds, env_indices, :]\n\n        data = (\n            self.observations[batch_inds, env_indices, :],\n            self.actions[batch_inds, env_indices, :],\n            next_obs,\n            # Only use dones that are not due to timeouts\n            # deactivated by default (timeouts is initialized as an array of False)\n            (self.dones[batch_inds, env_indices] * (1 - self.timeouts[batch_inds, env_indices])).reshape(-1, 1),\n            self.rewards[batch_inds, env_indices].reshape(-1, 1),\n        )\n        return ReplayBufferSamples(*tuple(map(self.to_torch, data)))\n\n    @staticmethod\n    def _maybe_cast_dtype(dtype: np.typing.DTypeLike) -> np.typing.DTypeLike:\n        \"\"\"\n        Cast `np.float64` action datatype to `np.float32`,\n        keep the others dtype unchanged.\n        See GH#1572 for more information.\n\n        :param dtype: The original action space dtype\n        :return: ``np.float32`` if the dtype was float64,\n            the original dtype otherwise.\n        \"\"\"\n        if dtype == np.float64:\n            return np.float32\n        return dtype\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:16:04.737707Z","iopub.execute_input":"2025-12-24T08:16:04.738835Z","iopub.status.idle":"2025-12-24T08:16:04.749706Z","shell.execute_reply.started":"2025-12-24T08:16:04.738805Z","shell.execute_reply":"2025-12-24T08:16:04.748821Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_sac():\n    args = Args()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    \n    # Initialize wandb\n    if args.track:\n        import wandb\n        wandb.init(\n            project=args.wandb_project_name,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    \n    writer = SummaryWriter(f\"runs/{run_name}\")\n    \n    # Seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Create environment\n    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n    \n    # Initialize networks\n    actor = Actor(envs).to(device)\n    qf1 = SoftQNetwork(envs).to(device)\n    qf2 = SoftQNetwork(envs).to(device)\n    qf1_target = SoftQNetwork(envs).to(device)\n    qf2_target = SoftQNetwork(envs).to(device)\n    qf1_target.load_state_dict(qf1.state_dict())\n    qf2_target.load_state_dict(qf2.state_dict())\n    \n    # Optimizers\n    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr, eps=1e-4)\n    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr, eps=1e-4)\n    \n    # Automatic entropy tuning\n    if args.autotune:\n        target_entropy = -args.target_entropy_scale * torch.log(1 / torch.tensor(envs.single_action_space.n))\n        log_alpha = torch.zeros(1, requires_grad=True, device=device)\n        alpha = log_alpha.exp().item()\n        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr, eps=1e-4)\n    else:\n        alpha = args.alpha\n    \n    # Replay buffer\n    rb = ReplayBuffer(args.buffer_size, envs.single_observation_space, envs.single_action_space, device, optimize_memory_usage=True,\n         handle_timeout_termination=False)\n    \n    start_time = time.time()\n    obs, _ = envs.reset(seed=args.seed)\n    \n    # Track metrics for progress monitoring\n    episode_rewards = []\n    last_100_rewards = []\n    best_reward = -float('inf')\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING TRAINING\")\n    print(\"=\"*60)\n    print(f\"Environment: {args.env_id}\")\n    print(f\"Total Timesteps: {args.total_timesteps:,}\")\n    print(f\"Device: {device}\")\n    print(f\"Learning starts at step: {args.learning_starts:,}\")\n    print(\"=\"*60 + \"\\n\")\n    \n    # Training loop\n    for global_step in range(args.total_timesteps):\n        if global_step % 1000 == 0 and global_step > 0:\n            elapsed = time.time() - start_time\n            sps = global_step / elapsed\n            remaining_steps = args.total_timesteps - global_step\n            eta_seconds = remaining_steps / sps if sps > 0 else 0\n            eta_hours = eta_seconds / 3600\n            \n            print(f\"‚è≥ Progress: {global_step:,}/{args.total_timesteps:,} ({100*global_step/args.total_timesteps:.1f}%) | \"\n                  f\"SPS: {sps:.0f} | ETA: {eta_hours:.1f}h\", end='\\r')\n            \n        # Select action\n        if global_step < args.learning_starts:\n            actions = np.array([envs.single_action_space.sample()])\n        else:\n            actions, _, _ = actor.get_action(torch.Tensor(obs).to(device))\n            actions = actions.detach().cpu().numpy()\n        \n        # Step environment\n        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n        dones = np.logical_or(terminations, truncations)\n        \n        # Log episode info - check for real episode completions\n        for idx in range(envs.num_envs):\n            if dones[idx]:\n                # Check if this was a real episode end (not just life loss)\n                if \"episode\" in infos:\n                    episode_return = infos[\"episode\"][\"r\"][idx]\n                    episode_length = infos[\"episode\"][\"l\"][idx]\n                    \n                    print(f\"\\n{'='*60}\")\n                    print(f\" EPISODE COMPLETED at Step {global_step:,}\")\n                    print(f\"{'='*60}\")\n                    print(f\" Return: {episode_return:.2f}\")\n                    print(f\"  Length: {episode_length}\")\n                    print(f\" Alpha (Entropy): {alpha:.4f}\")\n                    \n                    # Track rewards for monitoring improvement\n                    episode_rewards.append(episode_return)\n                    last_100_rewards.append(episode_return)\n                    if len(last_100_rewards) > 100:\n                        last_100_rewards.pop(0)\n                    \n                    avg_100 = np.mean(last_100_rewards)\n                    print(f\"üìà Avg Last 100 Episodes: {avg_100:.2f}\")\n                    \n                    if episode_return > best_reward:\n                        best_reward = episode_return\n                        print(f\"üèÜ NEW BEST REWARD: {best_reward:.2f}\")\n                    \n                    print(f\"{'='*60}\\n\")\n                    writer.add_scalar(\"charts/episodic_return\", episode_return, global_step)\n                    writer.add_scalar(\"charts/episodic_length\", episode_length, global_step)\n                    writer.add_scalar(\"charts/avg_100_return\", avg_100, global_step)\n        \n        # Store transition\n        real_next_obs = next_obs.copy()\n        for idx, trunc in enumerate(truncations):\n            if trunc:\n                real_next_obs[idx] = infos[\"final_observation\"][idx]\n        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n        \n        obs = next_obs\n        \n        # Training\n        if global_step > args.learning_starts and global_step % args.update_frequency == 0:\n            data = rb.sample(args.batch_size)\n            \n            # Critic update\n            with torch.no_grad():\n                _, next_state_log_pi, next_state_action_probs = actor.get_action(data.next_observations)\n                qf1_next_target = qf1_target(data.next_observations)\n                qf2_next_target = qf2_target(data.next_observations)\n                min_qf_next_target = next_state_action_probs * (\n                    torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n                )\n                min_qf_next_target = min_qf_next_target.sum(dim=1)\n                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target\n            \n            qf1_values = qf1(data.observations)\n            qf2_values = qf2(data.observations)\n            qf1_a_values = qf1_values.gather(1, data.actions.long()).view(-1)\n            qf2_a_values = qf2_values.gather(1, data.actions.long()).view(-1)\n            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n            qf_loss = qf1_loss + qf2_loss\n            \n            q_optimizer.zero_grad()\n            qf_loss.backward()\n            q_optimizer.step()\n            \n            # Actor update\n            _, log_pi, action_probs = actor.get_action(data.observations)\n            with torch.no_grad():\n                qf1_values = qf1(data.observations)\n                qf2_values = qf2(data.observations)\n                min_qf_values = torch.min(qf1_values, qf2_values)\n            actor_loss = (action_probs * ((alpha * log_pi) - min_qf_values)).mean()\n            \n            actor_optimizer.zero_grad()\n            actor_loss.backward()\n            actor_optimizer.step()\n            \n            # Alpha update\n            if args.autotune:\n                alpha_loss = (action_probs.detach() * (-log_alpha.exp() * (log_pi + target_entropy).detach())).mean()\n                a_optimizer.zero_grad()\n                alpha_loss.backward()\n                a_optimizer.step()\n                alpha = log_alpha.exp().item()\n            \n            # Update target networks\n            if global_step % args.target_network_frequency == 0:\n                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n            \n            # Logging\n            if global_step % 100 == 0:\n                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n                writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n                writer.add_scalar(\"losses/alpha\", alpha, global_step)\n                sps = int(global_step / (time.time() - start_time))\n                print(f\"SPS: {sps}\")\n                writer.add_scalar(\"charts/SPS\", sps, global_step)\n                if args.autotune:\n                    writer.add_scalar(\"losses/alpha_loss\", alpha_loss.item(), global_step)\n            # Detailed logging every 5000 steps\n            if global_step % 5000 == 0:\n                print(f\"\\n{'‚îÄ'*60}\")\n                print(f\" TRAINING METRICS at Step {global_step:,}\")\n                print(f\"{'‚îÄ'*60}\")\n                print(f\"Critic Loss (Q1): {qf1_loss.item():.4f}\")\n                print(f\"Critic Loss (Q2): {qf2_loss.item():.4f}\")\n                print(f\"Actor Loss: {actor_loss.item():.4f}\")\n                print(f\"Q1 Values (mean): {qf1_a_values.mean().item():.4f}\")\n                print(f\"Q2 Values (mean): {qf2_a_values.mean().item():.4f}\")\n                print(f\"Alpha: {alpha:.4f}\")\n                if args.autotune:\n                    print(f\"Alpha Loss: {alpha_loss.item():.4f}\")\n                print(f\"{'‚îÄ'*60}\\n\")\n    # Save model\n    torch.save({\n        'actor': actor.state_dict(),\n        'qf1': qf1.state_dict(),\n        'qf2': qf2.state_dict(),\n    }, f\"models/{run_name}.pt\")\n    \n    envs.close()\n    writer.close()\n\n    # Final summary\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETED!\")\n    print(\"=\"*60)\n    print(f\"Total Episodes: {len(episode_rewards)}\")\n    print(f\"Best Reward: {best_reward:.2f}\")\n    if len(last_100_rewards) > 0:\n        print(f\"Final Avg (Last 100): {np.mean(last_100_rewards):.2f}\")\n    print(f\"Total Time: {(time.time() - start_time)/3600:.2f} hours\")\n    print(f\"Model saved to: models/{run_name}.pt\")\n    print(f\"Videos saved to: videos/{run_name}/\")\n    print(\"=\"*60 + \"\\n\")\n\n    \n    print(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T07:59:09.226562Z","iopub.execute_input":"2025-12-24T07:59:09.226910Z","iopub.status.idle":"2025-12-24T07:59:09.259072Z","shell.execute_reply.started":"2025-12-24T07:59:09.226885Z","shell.execute_reply":"2025-12-24T07:59:09.257856Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"c64b239760de0f73ee8e1aa0987c6b6f93188607\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T08:09:47.595906Z","iopub.execute_input":"2025-12-24T08:09:47.596491Z","iopub.status.idle":"2025-12-24T08:09:57.834465Z","shell.execute_reply.started":"2025-12-24T08:09:47.596446Z","shell.execute_reply":"2025-12-24T08:09:57.833723Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n/usr/local/lib/python3.12/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return LooseVersion(v) >= LooseVersion(check)\n/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n  return datetime.utcnow().replace(tzinfo=utc)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamira-elgarf02\u001b[0m (\u001b[33mamira-elgarf02-cairo-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Run training\nif __name__ == \"__main__\":\n    train_sac()","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["wandb: Currently logged in as: amira-elgarf02 (amira-elgarf02-cairo-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.22.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>F:\\Uni\\fall2025\\RL\\Ass\\ass5\\wandb\\run-20251223_235618-gkipfx5a</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running/runs/gkipfx5a' target=\"_blank\">SpaceInvadersNoFrameskip-v4__sac_atari_spaceinvaders__1__1766526945</a></strong> to <a href='https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running/runs/gkipfx5a' target=\"_blank\">https://wandb.ai/amira-elgarf02-cairo-university/Assignment5-v3-running/runs/gkipfx5a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["F:\\Uni\\fall2025\\RL\\Ass\\ass5\\buffers.py:334: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 2.82GB > 0.52GB\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","============================================================\n","STARTING TRAINING\n","============================================================\n","Environment: SpaceInvadersNoFrameskip-v4\n","Total Timesteps: 5,000,000\n","Device: cuda\n","Learning starts at step: 10,000\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 407\n","============================================================\n"," Return: 110.00\n","  Length: 1661\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 110.00\n","üèÜ NEW BEST REWARD: 110.00\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 851\n","============================================================\n"," Return: 180.00\n","  Length: 1817\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 145.00\n","üèÜ NEW BEST REWARD: 180.00\n","============================================================\n","\n","‚è≥ Progress: 1,000/5,000,000 (0.0%) | SPS: 112 | ETA: 12.4h\n","============================================================\n"," EPISODE COMPLETED at Step 1,250\n","============================================================\n"," Return: 125.00\n","  Length: 1645\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 138.33\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 1,626\n","============================================================\n"," Return: 90.00\n","  Length: 1527\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 126.25\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 1,930\n","============================================================\n"," Return: 75.00\n","  Length: 1235\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 116.00\n","============================================================\n","\n","‚è≥ Progress: 2,000/5,000,000 (0.0%) | SPS: 156 | ETA: 8.9h\n","============================================================\n"," EPISODE COMPLETED at Step 2,375\n","============================================================\n"," Return: 105.00\n","  Length: 1813\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 114.17\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 2,815\n","============================================================\n"," Return: 105.00\n","  Length: 1791\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 112.86\n","============================================================\n","\n","‚è≥ Progress: 3,000/5,000,000 (0.1%) | SPS: 180 | ETA: 7.7h\n","============================================================\n"," EPISODE COMPLETED at Step 3,369\n","============================================================\n"," Return: 140.00\n","  Length: 2243\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 116.25\n","============================================================\n","\n","‚è≥ Progress: 4,000/5,000,000 (0.1%) | SPS: 198 | ETA: 7.0h\n","============================================================\n"," EPISODE COMPLETED at Step 4,165\n","============================================================\n"," Return: 345.00\n","  Length: 3205\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 141.67\n","üèÜ NEW BEST REWARD: 345.00\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 4,447\n","============================================================\n"," Return: 65.00\n","  Length: 1155\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 134.00\n","============================================================\n","\n","‚è≥ Progress: 5,000/5,000,000 (0.1%) | SPS: 214 | ETA: 6.5h\n","============================================================\n"," EPISODE COMPLETED at Step 5,154\n","============================================================\n"," Return: 345.00\n","  Length: 2878\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 153.18\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 5,522\n","============================================================\n"," Return: 30.00\n","  Length: 1519\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 142.92\n","============================================================\n","\n","‚è≥ Progress: 6,000/5,000,000 (0.1%) | SPS: 225 | ETA: 6.2h\n","============================================================\n"," EPISODE COMPLETED at Step 6,020\n","============================================================\n"," Return: 180.00\n","  Length: 2023\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 145.77\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 6,501\n","============================================================\n"," Return: 185.00\n","  Length: 1963\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 148.57\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 6,767\n","============================================================\n"," Return: 90.00\n","  Length: 1085\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 144.67\n","============================================================\n","\n","‚è≥ Progress: 7,000/5,000,000 (0.1%) | SPS: 230 | ETA: 6.0h\n","============================================================\n"," EPISODE COMPLETED at Step 7,286\n","============================================================\n"," Return: 120.00\n","  Length: 2117\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 143.12\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 7,798\n","============================================================\n"," Return: 110.00\n","  Length: 2097\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 141.18\n","============================================================\n","\n","‚è≥ Progress: 8,000/5,000,000 (0.2%) | SPS: 235 | ETA: 5.9h\n","============================================================\n"," EPISODE COMPLETED at Step 8,269\n","============================================================\n"," Return: 135.00\n","  Length: 1909\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 140.83\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 8,556\n","============================================================\n"," Return: 30.00\n","  Length: 1185\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 135.00\n","============================================================\n","\n","‚è≥ Progress: 9,000/5,000,000 (0.2%) | SPS: 236 | ETA: 5.9h\n","============================================================\n"," EPISODE COMPLETED at Step 9,003\n","============================================================\n"," Return: 135.00\n","  Length: 1821\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 135.00\n","============================================================\n","\n","\n","============================================================\n"," EPISODE COMPLETED at Step 9,790\n","============================================================\n"," Return: 255.00\n","  Length: 3171\n"," Alpha (Entropy): 1.0000\n","üìà Avg Last 100 Episodes: 140.71\n","============================================================\n","\n","SPS: 191ss: 10,000/5,000,000 (0.2%) | SPS: 239 | ETA: 5.8h\n","SPS: 187\n","SPS: 184\n","SPS: 181\n"]}],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}